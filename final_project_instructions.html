<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Final Project: Semiconductor Simulation Code Generation - CSC 375/575</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: Georgia, 'Times New Roman', serif;
            line-height: 1.7;
            color: #333;
            background-color: #ffffff;
            padding: 20px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background: white;
        }

        header {
            background-color: #003865;
            color: white;
            padding: 30px;
            margin-bottom: 30px;
            border-bottom: 4px solid #FDB913;
        }

        h1 {
            font-size: 2.2em;
            margin-bottom: 10px;
            border-bottom: 3px solid #FDB913;
            padding-bottom: 10px;
        }

        .subtitle {
            font-size: 1.1em;
            color: #FDB913;
            margin-top: 10px;
        }

        h2 {
            color: #003865;
            font-size: 1.5em;
            margin-top: 30px;
            margin-bottom: 15px;
            border-bottom: 2px solid #003865;
            padding-bottom: 5px;
        }

        h3 {
            color: #003865;
            font-size: 1.2em;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        p {
            margin-bottom: 15px;
        }

        strong {
            color: #003865;
        }

        .important-note {
            background-color: #fff8e1;
            border-left: 4px solid #FDB913;
            padding: 15px 20px;
            margin: 20px 0;
        }

        code {
            background-color: #f8f9fa;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #003865;
        }

        pre {
            background-color: #f8f9fa;
            border-left: 4px solid #003865;
            padding: 15px;
            overflow-x: auto;
            margin: 15px 0;
            white-space: pre-wrap;
        }

        pre code {
            background: none;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th {
            background-color: #003865;
            color: white;
            padding: 12px;
            text-align: left;
            border: 1px solid #dee2e6;
        }

        td {
            padding: 10px 12px;
            border: 1px solid #dee2e6;
        }

        tr:nth-child(even) {
            background-color: #f8f9fa;
        }

        ul, ol {
            margin-left: 30px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        .download-button {
            display: inline-block;
            padding: 12px 24px;
            background-color: #003865;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            margin: 10px 0;
            font-weight: bold;
        }

        .download-button:hover {
            background-color: #FDB913;
            color: #003865;
        }

        .back-button {
            display: inline-block;
            padding: 10px 20px;
            background-color: #003865;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            margin-top: 20px;
        }

        .back-button:hover {
            background-color: #FDB913;
            color: #003865;
        }

        .new-badge {
            background-color: #FDB913;
            color: #003865;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: bold;
            font-size: 0.85em;
            margin-left: 5px;
        }

        .new-section {
            border-left: 4px solid #FDB913;
            padding-left: 15px;
            margin-left: -19px;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Final Project: Semiconductor Simulation Code Generation</h1>
            <div class="subtitle">CSC 375/575 - Generative AI | Fall 2025</div>
            <div class="subtitle">Prof. Rongyu Lin, Quinnipiac University</div>
        </header>

        <section>
            <h2>Project Overview</h2>
            <p><strong>Goal:</strong> Build a language model system (≤1B parameters) to generate semiconductor device simulation code from natural language circuit design specifications, and <strong>design your own benchmark</strong> to evaluate model performance.</p>

            <p><strong>About the Simulation Platform:</strong> This project uses Silvaco TCAD (Technology Computer-Aided Design), an industry-standard semiconductor simulation platform for device modeling and circuit analysis. You will train models to generate SPICE-compatible simulation code that describes semiconductor device structures and electrical characteristics.</p>

            <p><strong>Possible Approaches:</strong> You can explore various techniques such as fine-tuning (LoRA, QLoRA), prompt engineering, retrieval-augmented generation (RAG), chain-of-thought prompting, or any combination that works best for your solution. The choice of methodology is completely open.</p>

            <p><strong>Benchmark Design <span class="new-badge">New</span>:</strong> You are responsible for designing a comprehensive benchmark to evaluate your model's code generation capabilities. This includes creating test cases, defining evaluation metrics, and demonstrating rigorous assessment of your model's strengths and weaknesses.</p>

            <p><strong>Format:</strong> Individual or team (2-3 students)</p>
            <p><strong>Final Presentation:</strong> December 3 (10 minutes per team)</p>
            <p><strong>Final Submission:</strong> December 12, 11:59 PM</p>
        </section>

        <section>
            <h2>Dataset</h2>

            <p><strong>Download:</strong></p>
            <a href="final_project/final_project_data.zip" class="download-button" download>Download Dataset (24 MB)</a>

            <p style="margin-top: 20px;"><strong>Contents:</strong></p>
            <ul>
                <li><code>silvaco_dataset_train.json</code> - 713 instruction-code pairs for training</li>
                <li><code>Silvaco_Examples_Student.zip</code> - 726 reference .in files + 76 .lib files</li>
                <li><code>README.md</code> - Complete dataset documentation</li>
            </ul>

            <div class="important-note">
                <p><strong>Important:</strong> You will design your own benchmark to evaluate your model. Focus on creating diverse, challenging test cases that assess generalization, not memorization.</p>
            </div>

            <div class="important-note" style="background-color: #fff3cd; border-left: 4px solid #856404;">
                <p><strong>Dataset Usage Restrictions:</strong></p>
                <ul style="margin-left: 20px; margin-bottom: 0;">
                    <li>This dataset is for <strong>CSC 375/575 course use only</strong></li>
                    <li><strong>Prohibited:</strong> Sharing, distributing, or publishing this dataset outside of this course</li>
                    <li><strong>Prohibited:</strong> Using this dataset for other projects, publications, or commercial purposes</li>
                    <li>Violation of these restrictions may result in academic penalties</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>Model Constraints</h2>

            <div class="important-note">
                <p><strong>CRITICAL:</strong> You must use models with <strong>≤1B parameters</strong>.</p>
            </div>

            <p><strong>Allowed models:</strong></p>
            <ul>
                <li>GPT-2 (117M-762M): gpt2, gpt2-medium, gpt2-large</li>
                <li>Llama 3.2 (1B): Llama-3.2-1B</li>
                <li>TinyLlama (1.1B): TinyLlama-1.1B</li>
                <li>DistilGPT-2 (82M): distilgpt2</li>
                <li>Qwen3 (0.6B): Qwen/Qwen3-0.6B-Base (Recommended - Latest 2025 model with strong multilingual support)</li>
                <li>T5 (60M-770M): t5-small, t5-base, t5-large</li>
                <li>BERT (110M-340M): bert-base, bert-large</li>
                <li>Any other pre-trained model ≤1B parameters</li>
            </ul>
        </section>

        <section>
            <h2>Benchmark Design Requirements <span class="new-badge">New</span></h2>

            <p>You must design a comprehensive benchmark to evaluate your model's code generation capabilities. Your benchmark should demonstrate thoughtful consideration of what makes good semiconductor simulation code.</p>

            <h3>Minimum Requirements</h3>
            <ul>
                <li><strong>Test Cases:</strong> At least 20 custom test cases (not from training data)</li>
                <li><strong>Diversity:</strong> Cover multiple device types, complexity levels, and edge cases</li>
                <li><strong>Metrics:</strong> Implement at least 3 evaluation metrics with clear justification</li>
                <li><strong>Documentation:</strong> Explain your benchmark design rationale</li>
            </ul>

            <h3>Suggested Evaluation Dimensions</h3>
            <ul>
                <li><strong>Syntax Correctness:</strong> Does the generated code follow valid SPICE/Silvaco syntax?</li>
                <li><strong>Semantic Accuracy:</strong> Does the code correctly implement the requested device/circuit?</li>
                <li><strong>Completeness:</strong> Are all required components and parameters included?</li>
                <li><strong>Code Quality:</strong> Is the code well-structured and follows best practices?</li>
                <li><strong>Edge Cases:</strong> How does the model handle unusual or complex requests?</li>
            </ul>

            <h3>Evaluation Metrics Examples</h3>
            <ul>
                <li>BLEU/ROUGE scores for code similarity</li>
                <li>Exact match accuracy for key parameters</li>
                <li>Syntax validation pass rate</li>
                <li>Component coverage score</li>
                <li>Custom domain-specific metrics</li>
            </ul>

            <div class="important-note">
                <p><strong>Key Point:</strong> The quality of your benchmark design is as important as your model's performance. A well-designed benchmark demonstrates deep understanding of the problem domain and rigorous evaluation methodology.</p>
            </div>
        </section>

        <section>
            <h2>Grading Rubric (100 Points Total)</h2>

            <table>
                <tr>
                    <th>Component</th>
                    <th>Points</th>
                    <th>Description</th>
                </tr>
                <tr>
                    <td>Benchmark Design & Evaluation <span class="new-badge">New</span></td>
                    <td>30</td>
                    <td>Quality and rigor of custom benchmark design, evaluation metrics, and results analysis</td>
                </tr>
                <tr>
                    <td>Implementation & Methodology</td>
                    <td>30</td>
                    <td>Training approach and technical implementation</td>
                </tr>
                <tr>
                    <td>Presentation</td>
                    <td>20</td>
                    <td>Live demonstration and explanation</td>
                </tr>
                <tr>
                    <td>Documentation & Code Quality</td>
                    <td>20</td>
                    <td>Technical report and code organization</td>
                </tr>
                <tr>
                    <td><strong>Total</strong></td>
                    <td><strong>100</strong></td>
                    <td></td>
                </tr>
            </table>

            <p><strong>Graduate students (CSC 575):</strong> Higher expectations for methodology sophistication, literature review, and analysis depth.</p>
        </section>

        <section>
            <h2>Deliverables</h2>

            <ol>
                <li><strong>Trained Model:</strong> Model weights and tokenizer (Hugging Face format preferred)</li>
                <li><strong>Custom Benchmark <span class="new-badge">New</span>:</strong>
                    <ul>
                        <li>Test dataset (at least 20 test cases in JSON format)</li>
                        <li>Evaluation scripts with implemented metrics</li>
                        <li>Benchmark design document explaining rationale and methodology</li>
                    </ul>
                </li>
                <li><strong>Code:</strong> Training scripts, data preprocessing, evaluation code</li>
                <li><strong>Technical Report (maximum 4 pages):</strong>
                    <ul>
                        <li>Model selection and justification</li>
                        <li>Training methodology and hyperparameters</li>
                        <li>Benchmark design and evaluation metrics</li>
                        <li>Results and analysis</li>
                        <li>Failure case analysis</li>
                    </ul>
                </li>
                <li><strong>Presentation (10 minutes):</strong> Live demo, methodology, benchmark results, Q&A</li>
                <li><strong>README:</strong> Setup instructions and usage guide</li>
            </ol>
        </section>

        <section>
            <h2>Submission</h2>

            <p>Submit via course website:</p>
            <ul>
                <li>All code and model files in a single archive</li>
                <li>Technical report (PDF)</li>
                <li>README with clear setup and usage instructions</li>
            </ul>

            <p><strong>Deadline:</strong> December 12, 11:59 PM</p>
        </section>

        <section>
            <h2>Academic Integrity</h2>

            <p><strong>Allowed:</strong></p>
            <ul>
                <li>Pre-trained models ≤1B from Hugging Face</li>
                <li>Standard libraries (transformers, PyTorch, TensorFlow)</li>
                <li>Data augmentation and preprocessing</li>
                <li>ChatGPT/Claude for debugging</li>
            </ul>

            <p><strong>Not Allowed:</strong></p>
            <ul>
                <li>Models >1B parameters</li>
                <li>Copying code between teams</li>
                <li>Using external code generation APIs or services</li>
                <li><strong>Sharing or distributing the course dataset</strong> outside of CSC 375/575</li>
            </ul>
        </section>

        <a href="https://rongyulin3.com/qu_genai/course.html" class="back-button">Back to Course Page</a>
    </div>
</body>
</html>
