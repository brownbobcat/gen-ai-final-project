{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "name": "Silvaco_Qwen_QLoRA",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Silvaco TCAD Code Generation \u2014 Colab Training Notebook\n",
        "This end-to-end notebook fine-tunes the \u22641B Qwen model with QLoRA on the Silvaco dataset, saves the adapter + tokenizer artifacts, and runs a quick generation smoke test so you can plug everything back into `src/generate.py`. Run each cell in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "workflow"
      },
      "source": [
        "**Workflow Overview**\n",
        "1. Verify GPU and install dependencies\n",
        "2. Upload your zipped project/data bundle (generated via `prepare_for_colab.sh` or manual zip)\n",
        "3. Locate the processed Hugging Face dataset + define training hyperparameters\n",
        "4. Fine-tune Qwen/Qwen2-0.5B with QLoRA\n",
        "5. Save the adapter + tokenizer, export a download zip\n",
        "6. Reload the checkpoint for an immediate Silvaco code generation sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpu_check"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \u26fd\ufe0f Check GPU availability\n",
        "!nvidia-smi\n",
        "import torch, platform\n",
        "print(f'Torch version: {torch.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    capability = torch.cuda.get_device_capability(0)\n",
        "    print(f'GPU: {device_name} (capability {capability[0]}.{capability[1]})')\n",
        "print(f'Python: {platform.python_version()}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83d\udce6 Install project dependencies\n",
        "!pip install -q -U \\n",
        "  transformers==4.40.2 \\n",
        "  accelerate==0.30.1 \\n",
        "  datasets==2.19.0 \\n",
        "  peft==0.11.1 \\n",
        "  bitsandbytes==0.43.1 \\n",
        "  sentence-transformers==3.0.1 \\n",
        "  evaluate==0.4.2 \\n",
        "  einops==0.7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_instr"
      },
      "source": [
        "## 1. Upload Your Project/Data Bundle\n",
        "Upload the archive produced by `silvaco_training_data.zip` (or zip the repo manually). The archive must contain at least:\n",
        "- `data/processed/processed_dataset/` (Hugging Face dataset from `src/preprocess.py`)\n",
        "- `src/` (so you can reference helper scripts later)\n",
        "- optional `embeddings/` or `model/` artifacts if you want to test RAG on Colab\n",
        "If you re-run this cell, the previous `/content/project` folder will be wiped to avoid stale files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upload_zip"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83d\udcc1 Upload and extract your zip file\n",
        "import shutil, zipfile, os\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "project_dir = Path('/content/project')\n",
        "if project_dir.exists():\n",
        "    shutil.rmtree(project_dir)\n",
        "project_dir.mkdir(parents=True, exist_ok=True)\n",
        "print('Select your project zip (e.g., silvaco_training_data.zip)...')\n",
        "uploaded = files.upload()\n",
        "for filename, data in uploaded.items():\n",
        "    local_path = Path(filename)\n",
        "    with open(local_path, 'wb') as f:\n",
        "        f.write(data)\n",
        "    if local_path.suffix == '.zip':\n",
        "        print(f'Extracting {local_path} ...')\n",
        "        with zipfile.ZipFile(local_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(project_dir)\n",
        "print('Extraction complete. Listing top-level contents:')\n",
        "for path in project_dir.iterdir():\n",
        "    print(' -', path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "detect_paths"
      },
      "source": [
        "## 2. Locate Project Root and Dataset\n",
        "This cell automatically detects the directory that contains `src/` and `data/`. Adjust the fallback paths if your archive uses a different layout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "set_paths"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83d\udd0d Detect important directories\n",
        "from pathlib import Path\n",
        "project_dir = Path('/content/project')\n",
        "def find_project_root(base: Path) -> Path:\n",
        "    candidates = []\n",
        "    for src_dir in base.rglob('src'):\n",
        "        root = src_dir.parent\n",
        "        if (root / 'data').exists():\n",
        "            candidates.append(root)\n",
        "    if candidates:\n",
        "        return max(candidates, key=lambda p: len(str(p)))\n",
        "    return base\n",
        "PROJECT_ROOT = find_project_root(project_dir)\n",
        "DATASET_PATH = PROJECT_ROOT / 'data' / 'processed' / 'processed_dataset'\n",
        "print(f'PROJECT_ROOT -> {PROJECT_ROOT}')\n",
        "print(f'DATASET_PATH -> {DATASET_PATH}')\n",
        "if not DATASET_PATH.exists():\n",
        "    raise FileNotFoundError('Processed dataset not found. Run src/preprocess.py locally and upload the resulting data/processed/processed_dataset folder.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_dataset"
      },
      "source": [
        "## 3. Load the Tokenized Dataset\n",
        "We reuse the Hugging Face dataset saved by `src/preprocess.py`. This avoids re-tokenizing large Silvaco files on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dataset_cell"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83d\udcda Load dataset and show stats\n",
        "from datasets import load_from_disk\n",
        "dataset = load_from_disk(str(DATASET_PATH))\n",
        "dataset_splits = list(dataset.keys())\n",
        "print('Available splits:', dataset_splits)\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['validation'] if 'validation' in dataset else None\n",
        "print(f'Train samples: {len(train_dataset)}')\n",
        "if eval_dataset is not None:\n",
        "    print(f'Validation samples: {len(eval_dataset)}')\n",
        "else:\n",
        "    print('Validation split missing -> creating 5% eval subset on-the-fly')\n",
        "    eval_dataset = train_dataset.train_test_split(test_size=0.05, seed=42)['test']\n",
        "print('Columns:', train_dataset.column_names)\n",
        "print('Example keys in a sample:', train_dataset[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_hparams"
      },
      "source": [
        "## 4. Configure QLoRA Training\n",
        "Tweak the hyperparameters below to trade off speed vs. quality. Smaller batch sizes help when running on T4/16GB GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "config_cell"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \u2699\ufe0f Hyperparameters\n",
        "MODEL_NAME = 'Qwen/Qwen2-0.5B'  #@param {type:'string'}\n",
        "OUTPUT_DIR = '/content/model_output'  #@param {type:'string'}\n",
        "MICRO_BATCH_SIZE = 2  #@param {type:'integer'}\n",
        "GRAD_ACCUM = 16  #@param {type:'integer'}\n",
        "NUM_EPOCHS = 1  #@param {type:'integer'}\n",
        "LEARNING_RATE = 0.0002  #@param {type:'number'}\n",
        "MAX_SEQ_LENGTH = 2048  #@param {type:'integer'}\n",
        "LORA_R = 16  #@param {type:'integer'}\n",
        "LORA_ALPHA = 32  #@param {type:'integer'}\n",
        "LORA_DROPOUT = 0.05  #@param {type:'number'}\n",
        "print('Configuration:')\n",
        "print(f'  Model: {MODEL_NAME}')\n",
        "print(f'  Output dir: {OUTPUT_DIR}')\n",
        "print(f'  Micro batch: {MICRO_BATCH_SIZE}, Grad accum: {GRAD_ACCUM}')\n",
        "print(f'  Epochs: {NUM_EPOCHS}, Learning rate: {LEARNING_RATE}')\n",
        "print(f'  LoRA r/alpha/dropout: {LORA_R}/{LORA_ALPHA}/{LORA_DROPOUT}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83e\udde0 Load tokenizer + base model with 4-bit quantization\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8 else torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4'\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM',\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trainer_setup"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83c\udfc3\u200d\u2642\ufe0f Prepare Trainer\n",
        "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type='cosine',\n",
        "    logging_steps=10,\n",
        "    save_strategy='epoch',\n",
        "    evaluation_strategy='epoch',\n",
        "    bf16=use_bf16,\n",
        "    fp16=not use_bf16,\n",
        "    max_grad_norm=1.0,\n",
        "    report_to='none',\n",
        "    optim='paged_adamw_32bit'\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "print('Trainer ready!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train_run"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83d\ude80 Launch training\n",
        "train_result = trainer.train()\n",
        "trainer.save_state()\n",
        "metrics = train_result.metrics\n",
        "print('Training metrics:', metrics)\n",
        "eval_metrics = trainer.evaluate()\n",
        "print('Evaluation metrics:', eval_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_section"
      },
      "source": [
        "## 5. Save, Zip, and Download the Adapter + Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83d\udcbe Persist artifacts\n",
        "from pathlib import Path\n",
        "adapter_dir = Path(OUTPUT_DIR) / 'adapter_model'\n",
        "tokenizer_dir = Path(OUTPUT_DIR) / 'tokenizer'\n",
        "adapter_dir.mkdir(parents=True, exist_ok=True)\n",
        "tokenizer_dir.mkdir(parents=True, exist_ok=True)\n",
        "trainer.model.save_pretrained(str(adapter_dir))\n",
        "tokenizer.save_pretrained(str(tokenizer_dir))\n",
        "trainer.save_metrics('train', metrics)\n",
        "trainer.save_metrics('eval', eval_metrics)\n",
        "print(f'Adapter saved to {adapter_dir}')\n",
        "print(f'Tokenizer saved to {tokenizer_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zip_download"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83d\udce6 Create download zip\n",
        "import shutil, os\n",
        "from google.colab import files\n",
        "zip_path = '/content/silvaco_trained_model.zip'\n",
        "if os.path.exists(zip_path):\n",
        "    os.remove(zip_path)\n",
        "shutil.make_archive('/content/silvaco_trained_model', 'zip', OUTPUT_DIR)\n",
        "print(f'Created {zip_path}')\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smoke_test_md"
      },
      "source": [
        "## 6. Reload Adapter for a Quick Generation Test\n",
        "This cell proves the weights you just trained can generate Silvaco code without leaving Colab. For full RAG + benchmarking, move the adapter/tokenizer back into your local repo and run `src/generate.py` / `benchmark/eval.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smoke_test"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83e\uddea Smoke test the fine-tuned model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from pathlib import Path\n",
        "adapter_dir = Path(OUTPUT_DIR) / 'adapter_model'\n",
        "tokenizer_dir = Path(OUTPUT_DIR) / 'tokenizer'\n",
        "assert adapter_dir.exists(), 'Adapter folder missing'\n",
        "smoke_tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, trust_remote_code=True)\n",
        "if smoke_tokenizer.pad_token is None:\n",
        "    smoke_tokenizer.pad_token = smoke_tokenizer.eos_token\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=dtype,\n",
        "    device_map='auto'\n",
        ")\n",
        "ft_model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
        "ft_model.eval()\n",
        "description = 'Design an NMOS transistor with 0.5um gate length, 10um width, and include DC sweep from 0-3V gate voltage.'\n",
        "prompt = (\n",
        "    'You are a semiconductor TCAD code generator.\\n\\n'\n",
        "    'Write a Silvaco ATLAS .in file based on the following device description:\\n'\n",
        "    f'{description}\\n\\n'\n",
        "    'Use correct Silvaco syntax with mesh, regions, electrodes, materials, doping, models, and solve steps.\\n\\n'\n",
        "    'Silvaco code:\\n'\n",
        ")\n",
        "inputs = smoke_tokenizer(prompt, return_tensors='pt').to(ft_model.device)\n",
        "with torch.no_grad():\n",
        "    output = ft_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=smoke_tokenizer.pad_token_id\n",
        "    )\n",
        "generated_text = smoke_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "generated_code = generated_text[len(prompt):]\n",
        "print(generated_code[:2000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## 7. Next Steps\n",
        "1. Download `silvaco_trained_model.zip` and extract it into your repo\u2019s `model/` directory.\n",
        "2. Run `python src/generate.py --input \"...\" --adapter_path model/adapter_model` locally (with optional `--rag`) to verify outputs.\n",
        "3. Execute `python benchmark/eval.py` to recompute metrics using the new adapter.\n",
        "4. Update the technical report + presentation with the refreshed benchmark results."
      ]
    }
  ]
}